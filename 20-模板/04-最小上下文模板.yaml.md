## 1）整体概要

这份 YAML 文件定义了一个名为 `minimal_context.yaml` 的“轻量级、可复用的 LLM 交互上下文模板”。其核心目标是为大型语言模型（LLM）的交互提供一个结构化的、可配置的上下文管理框架。它详细规定了构建一个高效且受控的 LLM 提示（prompt）所需的各个组成部分及其组织方式，旨在帮助开发者：



* **标准化 LLM 提示结构**：提供一个统一的蓝图，确保每次与 LLM 交互时都能以一致的方式构建上下文。

* **精细化 LLM 行为**：通过明确的系统指令（角色、能力、约束）、内存管理和少量示例，引导 LLM 按照预期进行响应。

* **优化 Token 使用**：通过预算限制、裁剪策略和优先级排序，有效管理 LLM 的输入长度，避免超出模型限制并控制成本。

* **支持交互式应用**：设计了记忆（Memory）模块，能够集成对话历史，使 LLM 具备多轮对话能力。

* **支持评估和维护**：定义了评估指标，为后续对 LLM 响应质量的量化评估提供了依据。



简而言之，这是一个“上下文工程”的 YAML 蓝图，用于指导如何有效地构建、管理和优化 LLM 的输入上下文，以实现更精准、高效和可控的模型行为。



## 2）模块说明



该 YAML 文件通过不同的顶级键（sections）组织了 LLM 上下文的各个方面：



* **`metadata` (元数据)**

  * **作用**：提供关于此上下文模板的基本信息。

  * **关键字段**：

    * `version`: 模板版本。

    * `description`: 模板的简要描述。

    * `author`: 模板的贡献者。

    * `token_budget`: 整个上下文的目标最大 Token 数量，用于指导 Token 管理策略。



* **`system` (系统指令)**

  * **作用**：定义 LLM 应扮演的角色、具备的能力和必须遵守的约束，这是控制 LLM 核心行为的关键部分。

  * **关键字段**：

    * `role`: LLM 扮演的角色，如 "assistant"、"math tutor" 等。

    * `capabilities` (列表)：LLM 应该具备的功能，例如“回答问题”、“解释概念”。

    * `constraints` (列表)：LLM 在响应时应遵守的限制或规则，例如“提供准确信息”、“避免不必要的冗长”。



* **`memory` (记忆/对话历史)**

  * **作用**：管理 LLM 与用户之间的对话历史，以实现多轮交互的上下文连贯性。

  * **关键字段**：

    * `enabled`: 布尔值，是否启用对话历史跟踪。

    * `max_turns`: 最多包含的上一轮对话的数量。

    * `pruning_strategy`: 当对话历史过长时，如何裁剪的策略（如 `drop_oldest` 丢弃最旧的、`summarize` 总结、`prioritize` 优先保留重要的）。

    * `format`: 对话历史的表示格式模板，例如 `Human: {human_message}\nAssistant: {assistant_message}`。



* **`examples` (少量示例 Few-Shot Examples)**

  * **作用**：提供少量的输入-输出示例，以通过示范来引导 LLM 的行为和响应风格。

  * **关键字段**：

    * `enabled`: 布尔值，是否包含示例。

    * `exchanges` (列表)：包含 `human` 和 `assistant` 消息对的示例列表。



* **`evaluation` (评估指标)**

  * **作用**：定义了用于衡量 LLM 响应质量的标准。

  * **关键字段**：

    * `metrics` (列表)：每个元素包含 `name` 和 `description`，描述了评估的维度，如“相关性”、“简洁性”、“准确性”。



* **`token_management` (Token 管理)**

  * **作用**：提供策略和优先级，用于在上下文接近 `token_budget` 时进行优化。

  * **关键字段**：

    * `reduction_strategies` (列表)：当 Token 超预算时可以采取的裁剪措施，例如“裁剪最旧的对话回合”、“压缩详细示例”。

    * `priority` (列表)：定义了上下文内容组件的优先级，高优先级的更可能被保留，例如“当前用户查询”最高，其次是“系统指令”、“最近对话历史”等。



* **`assembly` (上下文组装)**

  * **作用**：规定了如何将上述各个部分组合成最终发送给 LLM 的完整提示字符串。

  * **关键字段**：

    * `order` (列表)：定义了上下文组件在最终提示中的排列顺序。

    * `template`: 一个包含占位符的字符串模板，用于将不同部分的渲染内容插入到最终提示中。



## 3）代码的运行示例



```plain&#x20;text
--- YAML 模板文件 'minimal_context.yaml' 已创建 ---

1. 正在加载 YAML 模板文件: minimal_context.yaml
   YAML 模板加载成功。

2. 模拟用户查询和对话历史:
   当前用户查询: 'What is the capital of Japan?'
   对话历史回合数: 3

4. 正在组装最终的 LLM 提示上下文...

--- 最终生成的 LLM 提示上下文如下: ---
You are a helpful and knowledgeable assistant.
Capabilities: answering questions, explaining concepts, helping with tasks.
Constraints: provide accurate information, acknowledge uncertainty, avoid unnecessary verbosity.


Human: Hello, who are you?
Assistant: I am an AI assistant.

Human: What is the capital of France?
Assistant: The capital of France is Paris.

Human: And of Germany?
Assistant: The capital of Germany is Berlin.

Human: What is the capital of Japan?
Assistant:

--- 上下文生成完毕 ---

--- YAML 模板文件 'minimal_context.yaml' 已删除 ---
```

**说明：**

1. **文件创建**：示例首先将 YAML 内容写入一个临时文件 `minimal_context.yaml`，以便模拟真实场景中的文件加载。

2. **加载模板**：Python 代码使用 `yaml.safe_load()` 函数加载并解析这个 YAML 文件，将其转换为一个 Python 字典 `context_template`。

3) **模拟数据**：定义了 `current_user_query` 和 `conversation_history`，这些是动态变化的运行时数据。

4) **`assemble_context_from_template`** 函数：

   * 这是核心逻辑，它根据 YAML 模板的各个部分（`system`、`examples`、`memory`、`assembly`）来构建最终的提示字符串。

   * 它会根据 `enabled` 字段决定是否包含 `examples` 和 `memory`。

   * 它会根据 `memory.max_turns` 和 `pruning_strategy`（此处简化为取最近几轮）来处理历史对话。

   * 最终，它按照 `assembly.order` 和 `assembly.template` 将所有部分组合起来。

5. **最终提示**：打印出的 `final_prompt` 就是一个可以直接发送给 LLM 的完整上下文字符串。它包含了系统指令，最近的对话历史（因为 `examples` 被禁用，所以没有包含），以及当前的用户查询，并以 `Assistant:` 结尾，等待 LLM 补全其回答。



````yaml
# minimal_context.yaml
# A lightweight, reusable context template for LLM interactions
# ---------------------------------------------------------

# METADATA
# Basic information about this context template
metadata:
  version: "0.1.0"
  description: "Minimal viable context for general purpose LLM interactions"
  author: "Context Engineering Contributors"
  token_budget: 800  # Target maximum tokens for the entire context

# SYSTEM INSTRUCTIONS
# Core behavior and capabilities definition
system:
  role: "assistant"  # The role the LLM should adopt
  capabilities:
    - "answering questions"
    - "explaining concepts"
    - "helping with tasks"
  constraints:
    - "provide accurate information"
    - "acknowledge uncertainty"
    - "avoid unnecessary verbosity"
  
# MEMORY
# Essential state tracking for continuity
memory:
  # Set to true if you need to track conversation history
  enabled: true
  
  # Maximum number of previous exchanges to include
  max_turns: 3
  
  # Strategy for pruning conversation history when it gets too long
  pruning_strategy: "drop_oldest"  # Alternatives: summarize, prioritize
  
  # Format for representing conversation history
  format: |
    Human: {human_message}
    Assistant: {assistant_message}

# FEW-SHOT EXAMPLES
# Optional examples to guide the model's behavior
examples:
  enabled: false  # Set to true when you want to include examples
  
  # Format: List of human/assistant exchange pairs
  exchanges:
    - human: "What's the capital of France?"
      assistant: "The capital of France is Paris."
    
    - human: "How do I fix a leaky faucet?"
      assistant: "To fix a leaky faucet, first turn off the water supply. Then..."

# EVALUATION METRICS
# How to measure the quality of responses
evaluation:
  metrics:
    - name: "relevance"
      description: "How directly the response addresses the query"
      
    - name: "conciseness"
      description: "Appropriate length without unnecessary information"
      
    - name: "accuracy"
      description: "Factual correctness of the information provided"

# TOKEN MANAGEMENT
# Strategies for optimizing token usage
token_management:
  # When the context approaches the token budget, what to do
  reduction_strategies:
    - "Prune oldest conversation turns"
    - "Compress detailed examples"
    - "Remove optional context sections"
  
  # Priority order for content (highest first)
  priority:
    - "Current user query"
    - "System instructions"
    - "Recent conversation history"
    - "Few-shot examples"

# CONTEXT ASSEMBLY
# How to combine the components above into a complete context
assembly:
  order:
    - "system"
    - "examples" # Only if enabled
    - "memory"   # Only if enabled
    - "user_query"
  
  # A minimal template for assembling the context
  template: |
    {system}
    
    {examples}
    
    {memory}
    
    Human: {user_query}
    Assistant:

# USAGE EXAMPLE
# How to use this template in your code
# ----------------------------------
# 
# ```python
# import yaml
# 
# # Load the template
# with open('minimal_context.yaml', 'r') as f:
#     context_template = yaml.safe_load(f)
# 
# # Customize for your specific use case
# context_template['system']['role'] = "math tutor"
# context_template['token_budget'] = 500
# 
# # Assemble the context
# def assemble_context(template, user_query, conversation_history=None):
#     # Implementation details...
#     pass
# 
# # Use with your LLM
# prompt = assemble_context(context_template, "Help me solve 2x + 5 = 13")
# response = llm.generate(prompt)
# ```
````
